{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks: Logistic Regression and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Dataset from Berkson on Using Logistic Regression for Classification (1944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dose = [40, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300]\n",
    "exposed = [462, 500, 467, 515, 561, 469, 550, 542, 479, 497, 453]\n",
    "mortality = [109, 199, 298, 370, 459, 400, 495, 499, 450, 476, 442]\n",
    "dataset = list(zip(dose, exposed, mortality))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for obs in dataset:\n",
    "    mortality_nbr = obs[2]\n",
    "    survival_nbr = obs[1] - mortality_nbr\n",
    "    for _ in range(mortality_nbr):\n",
    "        X += [obs[0]]\n",
    "        y += [1]\n",
    "    for _ in range(survival_nbr):\n",
    "        X += [obs[0]]\n",
    "        y += [0]\n",
    "\n",
    "#X = list(map(log10, X))\n",
    "X = np.array(X).reshape(-1, 1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.hstack((X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Logistic Curve (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X, y, epochs=50, verbose=0)\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
    "plt.title('Training accuracy and loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training accuracy and loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crossentropy loss is defined as:\n",
    "$\n",
    "H(P,M) = - \\frac{1}{|X|} \\sum\\limits_{x \\in X} {P(x)\\log M(x),} \n",
    "$\n",
    "\n",
    "First, a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "random_observations = np.random.choice(X.shape[0], 6, replace = False)\n",
    "print(random_observations)\n",
    "X_choice = list(X[random_observations])\n",
    "print('Dose:', X_choice)\n",
    "y_choice = list(y[random_observations])\n",
    "print('Observed class:', y_choice)\n",
    "probs_raw = list(map(model.predict, X_choice))\n",
    "print('Predicted probs for class 1:', probs_raw)\n",
    "probs = [log(p) * y + log(1 - p) * (1 - y) for p, y in zip(probs_raw, y_choice)]\n",
    "print('Crossentropy per observation:', probs)\n",
    "entropy = -sum(probs)/6\n",
    "print('Crossentropy:', entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute it over the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_raw = list(map(model.predict, X))\n",
    "probs = [log(p) * y + log(1 - p) * (1 - y) for p, y in zip(probs_raw, y)]\n",
    "sum(probs)/X.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
