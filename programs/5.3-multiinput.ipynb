{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multi Input Network for Named Entity Recognition\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the lab on named entity recognition, we used the words to predict the named entities. We will now use two inputs: The words and the parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vilde = False\n",
    "if vilde:\n",
    "    BASE_DIR = '/home/pierre/Cours/EDAN20/corpus/CoNLL2003/'\n",
    "else:\n",
    "    BASE_DIR = '/Users/pierre/Projets/Corpora/CoNLL2003/'\n",
    "\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'NER-data/eng.train'\n",
    "    dev_file = BASE_DIR + 'NER-data/eng.valid'\n",
    "    test_file = BASE_DIR + 'NER-data/eng.test'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dictorizer that transforms the CoNLL files into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "print(train_dict[0])\n",
    "print(train_dict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function to build the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build the words (one of the inputs) and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words, Y_ner = build_sequences(train_dict, key_x='form', key_y='ner')\n",
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build the POS (the second input) and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos, Y_ner = build_sequences(train_dict, key_x='ppos', key_y='ner')\n",
    "print('First sentence, POS', X_pos[1])\n",
    "print('First sentence, NER', Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now extract the list of unique words, POS, and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = sorted(list(set([item for sublist in X_words for item in sublist])))\n",
    "pos_set = sorted(list(set([item for sublist in X_pos for item in sublist])))\n",
    "ner_set = sorted(list(set([item for sublist in Y_ner for item in sublist])))\n",
    "print(len(word_set))\n",
    "print(len(pos_set))\n",
    "print(pos_set)\n",
    "print(len(ner_set))\n",
    "ner_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_word_idx = dict(enumerate(word_set, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "rev_pos_idx = dict(enumerate(pos_set, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "pos_idx = {v: k for k, v in rev_pos_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the matrices\n",
    "We convert the matrices into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before: We have the symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_words[1])\n",
    "print(X_pos[1])\n",
    "Y_ner[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_idx = [list(map(lambda x: pos_idx.get(x, 1), x)) for x in X_pos]\n",
    "X_words_idx = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_words]\n",
    "Y_ner_idx = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_ner]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After: We have the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_words_idx[1])\n",
    "print(X_pos_idx[1])\n",
    "Y_ner_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_idx = pad_sequences(X_pos_idx)\n",
    "X_words_idx = pad_sequences(X_words_idx)\n",
    "Y_ner_idx = pad_sequences(Y_ner_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_words_idx[1])\n",
    "print(X_pos_idx[1])\n",
    "Y_ner_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create one encodings for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ner_idx_cat = to_categorical(Y_ner_idx)\n",
    "print(Y_ner_idx_cat[1])\n",
    "print(Y_ner_idx_cat[1][0])\n",
    "Y_ner_idx_cat[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabulary_size = len(word_set) + 2\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "embedded_text = layers.Embedding(text_vocabulary_size, \n",
    "                                 64, mask_zero=True)(text_input)\n",
    "encoded_text = layers.LSTM(32, return_sequences=True)(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The POS input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vocabulary_size = len(pos_set) + 2\n",
    "pos_input = Input(shape=(None,),\n",
    "                       dtype='int32',\n",
    "                       name='pos')\n",
    "embedded_pos = layers.Embedding(pos_vocabulary_size, \n",
    "                                32, mask_zero=True)(pos_input)\n",
    "encoded_pos = layers.LSTM(16, return_sequences=True)(embedded_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We concatenate the two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = layers.concatenate([encoded_text, encoded_pos],axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vocabulary_size = len(ner_set) + 2\n",
    "ner = layers.Dense(ner_vocabulary_size,\n",
    "                      activation='softmax')(concatenated)\n",
    "\n",
    "model = Model([text_input, pos_input], ner)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'text': X_words_idx, 'pos': X_pos_idx}, Y_ner_idx_cat,\n",
    "                  epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
