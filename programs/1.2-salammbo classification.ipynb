{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dataset: *Salammbô* in French and English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts of letters and _A_ broken down by chapter in *Salammbô* in French and its translation in English. The $\\mathbf{X}$ matrix contains the counts and $\\mathbf{y}$ the language; French: 1 and English: 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.array(\n",
    "    [[35680, 2217], [42514, 2761], [15162, 990], [35298, 2274],\n",
    "     [29800, 1865], [40255, 2606], [74532, 4805], [37464, 2396],\n",
    "     [31030, 1993], [24843, 1627], [36172, 2375], [39552, 2560],\n",
    "     [72545, 4597], [75352, 4871], [18031, 1119], [36961, 2503],\n",
    "     [43621, 2992], [15694, 1042], [36231, 2487], [29945, 2014],\n",
    "     [40588, 2805], [75255, 5062], [37709, 2643], [30899, 2126],\n",
    "     [25486, 1784], [37497, 2641], [40398, 2766], [74105, 5047],\n",
    "     [76725, 5312], [18317, 1215]\n",
    "     ], dtype='float64')\n",
    "\n",
    "y = np.array(\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_english, X_french = np.split(X, 2)\n",
    "\n",
    "en = plt.scatter(X_english[:,0], X_english[:,1], c='r', marker='x')\n",
    "fr = plt.scatter(X_french[:,0], X_french[:,1], c='b', marker='x')\n",
    "plt.title(\"Salammbô\")\n",
    "plt.xlabel(\"Letter count\")\n",
    "plt.ylabel(\"$A$ count\")\n",
    "plt.legend((en, fr), ('English', 'French'), loc='lower right', scatterpoints=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably possible to determine the language using the counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Larger Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extend the dataset to cover all the paragraphs in the novel, French and English, where we will count all the letters. Paragraph will be defined the newline character: `\\n`.<br/> Structure of the dataset:<br/>\n",
    "\n",
    "|Obs.#   | $\\mathbf{X}$                              | $\\mathbf{y}$  |\n",
    "|-------|----------------------------------|--------|\n",
    "|para_1 | a_counts, b_counts, c_counts, ...| English|\n",
    "|para_2 |a_counts, b_counts, c_counts, ... |English |\n",
    "|...    |...                               |...    |\n",
    "|para_1000| a_counts, b_counts, c_counts, ...|French|\n",
    "|para_1001| a_counts, b_counts, c_counts, ...| French|\n",
    "|...      |...                               |...     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function to Count Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(string):\n",
    "    counts = {}\n",
    "    for char in string:\n",
    "        if char.isalpha():\n",
    "            if char in counts:\n",
    "                counts[char] += 1\n",
    "            else:\n",
    "                counts[char] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Extract the Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salammbo_en = open('../../EDAN20/programs/corpus/Salammbo/salammbo_en.txt').read().strip().lower()\n",
    "salammbo_fr = open('../../EDAN20/programs/corpus/Salammbo/salammbo.txt').read().strip().lower()\n",
    "\n",
    "para_en = filter(None, salammbo_en.split('\\n'))\n",
    "para_fr = filter(None, salammbo_fr.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(None, salammbo_fr.split('\\n')))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_en = list(filter(None, map(count_chars, para_en)))\n",
    "counts_fr = list(filter(None, map(count_chars, para_fr)))\n",
    "counts_fr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts_en + counts_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Formatting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We associate the characters with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dict_vec = DictVectorizer(sparse=False)\n",
    "dict_vec.fit(counts)\n",
    "dict_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the dictionaries into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = dict_vec.transform(counts_en)\n",
    "X_fr = dict_vec.transform(counts_fr)\n",
    "print(X_en.shape)\n",
    "print(X_fr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stack the French and English datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_en, X_fr))\n",
    "y = [0] * len(counts_en) + [1] * len(counts_fr)\n",
    "X[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We shuffle the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(X.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "print(indices[:10])\n",
    "X = X[indices, :]\n",
    "y = np.array(y)[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_examples, :]\n",
    "y_train = y[:training_examples]\n",
    "\n",
    "X_val = X[training_examples:, :]\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the dataset. This is very significant for the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "std.fit(X_train)\n",
    "\n",
    "X_train = std.transform(X_train)\n",
    "X_val = std.transform(X_val)\n",
    "X_train[:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple architecture. This is simply logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(1, input_dim=X.shape[1], activation='sigmoid')])\n",
    "\n",
    "# Fitting the network\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_val)\n",
    "print(y_predicted[:10])\n",
    "print(y_val[:10])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_val, y_val)\n",
    "print('Scores:', scores)\n",
    "\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = []\n",
    "for i in y_predicted:\n",
    "    if i < 0.5:\n",
    "        y_pred.append([0])\n",
    "    else:\n",
    "        y_pred.append([1])\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complex Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a more complex model with two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(20, input_dim=X.shape[1], activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Fitting the network\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, \n",
    "                    validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_val)\n",
    "print(y_predicted[:10])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_val, y_val)\n",
    "print(scores)\n",
    "\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in y_predicted:\n",
    "    if i < 0.5:\n",
    "        y_pred.append([0])\n",
    "    else:\n",
    "        y_pred.append([1])\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a real system, see the _Compact Language Detector v3_: https://github.com/google/cld3\n",
    "and a reimplementation: https://github.com/pnugues/language-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
