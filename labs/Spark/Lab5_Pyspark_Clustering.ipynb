{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Word Counting and Embedding Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Marcus Klang (Marcus.Klang@cs.lth.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals:\n",
    " * Learn how to read, transform and process text data with Pyspark\n",
    " * Preprocess and create a suitable dataset for clustering\n",
    " * Use KMeans from sklearn and cluster 10 000 words to 200 clusters\n",
    " * Write a function which displays words nearby\n",
    " \n",
    "## Outline of the lab\n",
    "\n",
    " * You will first solve a few exercises on Spark to learn how to write basic commands.\n",
    " * You will then apply Spark to extract the 10,000 most frequent words in the English Wikipedia.\n",
    " * As this corpus is very large, you will use 1% of it in the lab, the full Wikipedia is available.\n",
    " * You will finally cluster these words into 100 groups using their GloVe100 representation.\n",
    " * As clustering program, you will use KMeans from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When a cell contains TODO, you shall replace TODO with relevant code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 0**: Add 1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and Pyspark documentation\n",
    "\n",
    "General documention is available online when you need to look up the parameters of a function:\n",
    " \n",
    " * [Pyspark RDD](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html)\n",
    " * [Pyspark SQL](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html)\n",
    " * [Spark](https://spark.apache.org/docs/2.4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_6B_100D_PATH=\"\"\n",
    "ENWIKI_1P_PATH=\"/usr/local/cs/EDAN95/datasets/wikipedia/enwiki_1p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy glove.6B.100d.txt from [Assignment 4](http://cs.lth.se/edan95-applied-machine-learning/lab-programming-assignments/assignment-4/) and set path above\n",
    "\n",
    "**If you are on your own computer:**\n",
    "\n",
    "1. Download the 1% Enwiki Paragraph dataset from http://fileadmin.cs.lth.se/cs/Education/EDAN95/Data/enwiki_1p.zip \n",
    "2. Extract relative to notebook\n",
    "3. Make sure the relative directory contains \"part-*\" files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the computers in the lab rooms then everything is ready.\n",
    "\n",
    "**On your own computer:**\n",
    "\n",
    "These instructions are given in the hope that they are useful and help get you started, support will be minimal to none:\n",
    "\n",
    " 1. [Download Spark (Mirror choser - get real link from this page)](https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz), link from [https://spark.apache.org/]\n",
    " 2. Unpack this directory, copy the path to this directory\n",
    " 3. Make sure you have the [Java 8 **JDK**](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) installed and it is the default java. **Spark does not support Java 9+**, if your default java is 9+, then you have to set and export the environment variable JAVA_HOME with the proper location of your java 8 installation before starting your notebook/lab server, which implies that you need to restart the server running this notebook.\n",
    " 4. Modify SPARK_HOME in the cell below with the copied path and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path should point to a location which contains a bin and python directory\n",
    "SPARK_HOME=\"!! REPLACE THIS WITH THE PATH TO THE UNPACKED DIRECTORY !!\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\"))\n",
    "sys.path.append(os.path.join(SPARK_HOME, \"python\", \"lib\", \"py4j-0.10.7-src.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Pyspark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext   # The main context\n",
    "from pyspark.sql import SQLContext # The SQL CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utility function will resolve a correct path for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_path(filepath):\n",
    "    return \"file:%s\" % os.path.abspath(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a ```SparkContext``` for testing purposes i.e. run it locally and use all available cores.\n",
    "\n",
    "```local[*]```, the ```*``` defines the number of cores to use (* = all, 1 = one core, 4 = four cores, etc.).\n",
    "\n",
    "This will create the Py4j bridge, and start a background JVM which runs the Spark Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master=\"local[*]\", appName=\"Lab 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```sc``` variable will provide a link to the driver webpage where status information can be found, along with all settings and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Tiny steps with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this section is to get you familiar with different transformations and actions.\n",
    "\n",
    "All tasks will contain links to the function to use when a new concepts are introduced, when no links exists you are expected to use suitable functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some dummy data, 100 000 elements from 0 to 99 999, use 8 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sc.parallelize(list(range(100000)), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Print the 5 first elements, use the [take](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.take) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Use [reduce](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.reduce) to sum all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Sum the values using the [sum](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.sum) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Compute the mean using the [mean](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.mean) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Add 15 to all elements using [map](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.map), take the first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Repeat task 5 but use a function instead of a lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** Filter out odd numbers (divisble by 2 is ```x % 2 == 0```) and compute the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8:** Find all odd numbers between 10 000 and 50 000 (inclusive), add 134, find all divisible by 17, [collect](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.collect) and [count](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.count) them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 The execution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark uses multiple processes for parellization as CPython lacks native threading.\n",
    "\n",
    "Although possible, shared memory is not used, which means all processes are unique and data which resides in the driver is copied when needed, and state is not transfered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    random.seed(1)\n",
    "    print(dataset.map(lambda x: x+random.randint(0,100)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exepected output above is 3 different values, as the random state from the driver is not copied to the executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will attempt to make it predictable, by seeding in each process and using a function instead of a lambda.\n",
    "\n",
    "In addition, the code below will not return all values, instead it will compute a single sum within each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictable_map_partition(indx, partition):\n",
    "    # Whatever code you write here, will run in the executor\n",
    "    random.seed(indx)\n",
    "    return [sum(x+random.randint(0,100) for x in partition)]\n",
    "\n",
    "for i in range(3):\n",
    "    print(dataset.mapPartitionsWithIndex(predictable_map_partition).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of above is predictable as each partition will be seeded in a predictable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3 Sharing read-only data between executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not uncommon to need some data to be shared between executors.\n",
    "\n",
    "Common use cases are:\n",
    " * Sets/Dictionaries for fast in-memory lookup\n",
    " * User-defined objects which contains parameters for the algorithm that should be parallelized\n",
    " * General in-memory objects which are used frequently but never modified. These objects might have expensive intialization (such as loading a model, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is shared using a mechanism called *[Broadcasts](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.Broadcast)*, these are serialized objects which are distributed efficiently and shared across muliple calls within a pipeline.\n",
    "\n",
    "```pickle```is used to serialize objects to transmit over network or locally to executor processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = sc.broadcast({\"a\": \"x\", \"b\":\"y\", \"c\": \"z\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([\"abc\", \"def\"],1).map(lambda x: \"\".join([lookup.value.get(ch,ch) for ch in x])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setlookup = sc.broadcast(set([\"def\"]))\n",
    "sc.parallelize([\"abc\", \"def\"],1).filter(lambda x: x in setlookup.value).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a 1% sample of all paragraphs in English Wikipedia from October 2018.\n",
    "\n",
    "The data has been segmented in advance, the following specifics apply:\n",
    " \n",
    " * Tokens/Words are seperated by space ' '\n",
    " * Sentences are seperated by tab characters, '\\t'\n",
    " * Paragraphs are seperated by newlines, which means that each entry is a full paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = sc.textFile(pyspark_path(ENWIKI_1P_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9**: Display the first 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10.** Count the number of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11:** [Sample](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.sample) 1% of these paragraphs, and display the first 5\n",
    "\n",
    "The use-case of sampling is to speed up testing, getting a downsampled version of the dataset retains the statistical properties.\n",
    "\n",
    "Please use sampling if execution times are too long. Another tip is to cache the 1% sample in memory for even better performance using [cache()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.cache), more fingrained caching can be controlled using [persist()](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.persist) and [StorageLevels](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.StorageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this part is to count words by doing the following parts:\n",
    "    \n",
    " * Convert paragraphs into tokens\n",
    " * Normalize tokens into lowercase\n",
    " * Count per token\n",
    " * Sort and retrieve the top 10000 words in the provided dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12**: Split paragraphs into tokens using [flatMap](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.flatMap), display first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 13:** Lower-case the tokens, display first 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remember* you can use a 1% sample during testing\n",
    "\n",
    "**Task 14:** Count tokens using *map* and [reduceByKey](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.reduceByKey), filter out ```sweden, lund, sk√•ne, learning, spark```, and display counts for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 15:** Count the total number of tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 16:** Take the top 10 000 most frequent tokens using [takeOrdered](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.takeOrdered).\n",
    "\n",
    "Save the result in a variable called ```top10000```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Process Glove, and extract the embeddings for the top 10000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this part is:\n",
    "\n",
    " * Parse the Glove 6B embedding\n",
    " * Filter out only the embeddings matching words that exists in the top 10 000 words.\n",
    " * Produce the X which we will cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 17:** Load Glove 6B 100d embeddings into a variable ```glove6B```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove6B = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 18:** Count the number of embeddings and print the first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 18:** Parse the embedding by using a function which splits the input into tuples of (word, embedding), display the first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 19:** Filter out the top 10000 embeddings using the provided broadcast ```top10000_set``` which uses the previous ```top10000```variable and use [collectAsMap](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD.collectAsMap) to get a dictionary of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10000_set = sc.broadcast(set([word for word, cnt in top10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10000_emb = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all 10 000 words exists in Glove, **9 962** is the expected count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top10000_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 20:** Produce the X that we will cluster.\n",
    "\n",
    "1. Produce a sorted list of tokens\n",
    "2. Transform list into a list of embeddings and create a numpy array from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cluster X using KMeans and visualize the results\n",
    "\n",
    " * Cluster these embeddings into 100 clusters using [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) from scikit-learn\n",
    " * Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, n_jobs=-1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the score which follows this equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "-\\sum_{i=1}^n (\\sum_{k=1}^{100} (x_{i,k} - {xc}_{i,k})^2)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $x$ is the embedding, $xc$ is the assigned cluster center.\n",
    "\n",
    "*In words:* It is the negative sum of squared differences per dimension (100d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 21:** Predict clusters, the output will be indicies. You will find how in the documentation of [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clusters = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clusters[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 22:** Create mapping from word to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 23:** Create mapping from cluster to all words in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 24**: Create a list of all cluster sizes, i.e. the number of words in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(cluster_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optionally:** Look at the words in the smallest and largest cluster, do you see any structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 25:** Implement the get_nearby function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearby(word):\n",
    "    \"\"\"\n",
    "    Get all words from the same cluster as the input\n",
    "    \n",
    "    Returns: list of words having the same cluster\"\"\"\n",
    "    \n",
    "    return #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nearby(\"sweden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"king\", \"sweden\", \"obama\", \"home\", \"learning\"]\n",
    "clusters = [get_nearby(word) for word in words]\n",
    "cluster_colors = [\"red\", \"green\", \"blue\", \"purple\", \"teal\"] # HTML colors are allowed inside here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten words and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for cluster in clusters for word in cluster]\n",
    "all_cluster_color = [\n",
    "    cluster_color for cluster_color_array in \n",
    "    (\n",
    "        [color] * len(cluster)\n",
    "        for cluster, color in \n",
    "        zip(clusters, cluster_colors)\n",
    "    )\n",
    "    for cluster_color\n",
    "    in cluster_color_array\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 26:** Use ```all_words``` and translate into a numpy array called ```X_words``` of all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**TSNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a dimensionality reduction algorithm useful for projecting high dimensional data into a low dimensional space such as 2D, which we can plot and more easily understand spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = tsne.fit_transform(X_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below makes a large plot using Matplotlib and attaches labels to all points. It provided for reference and a fallback if the more advanced visualization fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using matplotlib and produce an image of the points and words\n",
    "labels = all_words\n",
    "x = []\n",
    "y = []\n",
    "for x_val,y_val in X_pos:\n",
    "    x.append(x_val)\n",
    "    y.append(y_val)\n",
    "\n",
    "plt.figure(figsize=(32, 32)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization: D3.js\n",
    "The cells below will produce a HTML file that has the previously plotted image embedded as JSON.\n",
    "\n",
    "The code below is provided for visualization purposes: to understand it is not a requirement.\n",
    "\n",
    "The standard output is ```visualize.html```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to fit box of dimensions below\n",
    "WIDTH = 768.0\n",
    "HEIGHT = 768.0\n",
    "OUTPUT_FILE = \"visualize.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set the template\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<head>\n",
    "  <style>\n",
    "\n",
    "      html {\n",
    "        font-family: \"Helvetica Neue\", Helvetica, arial, sans-serif;\n",
    "        font-size: 16px;\n",
    "      }\n",
    "\n",
    "      body {\n",
    "        margin: 0;\n",
    "        padding: 0;\n",
    "        width: 100%;\n",
    "        width: 100%;\n",
    "        overflow: hidden;\n",
    "      }\n",
    "\n",
    "      svg {\n",
    "        display: block;\n",
    "        width: 100%;\n",
    "        height: auto;\n",
    "      }\n",
    "\n",
    "      div#container {\n",
    "        overflow: hidden;\n",
    "      }\n",
    "    </style>\n",
    "    <meta charset=\"utf-8\">\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "<svg width=\"640\" height=\"480\"></svg>\n",
    "<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
    "\n",
    "<!-- Here is our data -->\n",
    "<script id=\"raw-data\" language=\"application/json\">\n",
    "__OUTPUT_JSON__\n",
    "</script>\n",
    "<script>\n",
    "\n",
    "// Parse data from tag inside this doucment\n",
    "var inputdata = JSON.parse(document.getElementById('raw-data').innerHTML);\n",
    "\n",
    "var svg = d3.select(\"svg\"),\n",
    "    width = +svg.attr(\"width\"),\n",
    "    height = +svg.attr(\"height\");\n",
    "\n",
    "// Create a dummy array with indicies\n",
    "var indicies = d3.range(inputdata.length);\n",
    "\n",
    "// Produce points with some zoom magic\n",
    "var circle = svg.selectAll(\"circle\")\n",
    "  .data(indicies)\n",
    "  .enter()\n",
    "  .append(\"circle\")\n",
    "    .attr(\"r\", 2.5)\n",
    "    .attr(\"fill\", function(d) {return inputdata[d][\"color\"]})\n",
    "    .attr(\"transform\", transform(d3.zoomIdentity));\n",
    "\n",
    "// Produce labels attached/placed relative to points\n",
    "var lbls = svg.selectAll(\"texts\")\n",
    "                .data(indicies)\n",
    "                .enter()\n",
    "                .append(\"text\")\n",
    "                .text(function(d) {\n",
    "                  return inputdata[d][\"label\"];\n",
    "                })\n",
    "                .attr(\"font-size\", \"15px\")\n",
    "                .attr(\"fill\", function(d) {return inputdata[d][\"color\"]})\n",
    "                .attr(\"transform\", transform(d3.zoomIdentity));\n",
    "\n",
    "// Will recieve index, and produce transform directive for that index\n",
    "function transform(t) {\n",
    "  return function(d) {\n",
    "    return \"translate(\" + t.apply(inputdata[d][\"position\"]) + \")\";\n",
    "  };\n",
    "}\n",
    "\n",
    "// Zoom transformation function\n",
    "function zoom() {\n",
    "  circle.attr(\"transform\", transform(d3.event.transform));\n",
    "  lbls.attr(\"transform\", transform(d3.event.transform));\n",
    "}\n",
    "\n",
    "// Add the zoom handler, that listens to events.\n",
    "svg.append(\"rect\")\n",
    "    .attr(\"fill\", \"none\")\n",
    "    .attr(\"pointer-events\", \"all\")\n",
    "    .attr(\"width\", \"100%\")\n",
    "    .attr(\"height\", \"100%\")\n",
    "    .call(d3.zoom()\n",
    "        .scaleExtent([1, 16]) //The minimum and maximum scale factor \n",
    "        .on(\"zoom\", zoom));\n",
    "\n",
    "</script>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find extents\n",
    "x_extents = [min(pos[0] for pos in X_pos), max(pos[0] for pos in X_pos)]\n",
    "y_extents = [min(pos[1] for pos in X_pos), max(pos[1] for pos in X_pos)]\n",
    "\n",
    "# Compute translation\n",
    "translation = [-x_extents[0], -y_extents[0]]\n",
    "\n",
    "# Compute scaling, flip y to match convention that y is down not up.\n",
    "scale = [WIDTH/(x_extents[1]-x_extents[0]), -HEIGHT/(y_extents[1]-y_extents[0])]\n",
    "\n",
    "# Produce JSON dictionary\n",
    "output = []\n",
    "labels = all_words\n",
    "for (x_val, y_val), label, color in zip(X_pos, labels, all_cluster_color):\n",
    "    output.append({\"position\": ((float(x_val)+translation[0])*scale[0], (float(y_val)+translation[1])*scale[1]+HEIGHT), \"label\": label, \"color\": color})\n",
    "\n",
    "# Write HTML file with JSON embedded\n",
    "with open(OUTPUT_FILE, \"w\") as fout:\n",
    "    fout.write(HTML_TEMPLATE.replace(\"__OUTPUT_JSON__\", json.dumps(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the HTML file in your favorite browser.\n",
    "\n",
    "How to use:\n",
    " * Drag to move the image around\n",
    " * Zoom using the mousewheel, default min and max zoom values have been set.\n",
    "\n",
    "**Note:** *Internet access is required* for the page to work. The page downloads d3.js during start-up from https://d3js.org/d3.v4.min.js"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
